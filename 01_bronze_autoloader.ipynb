{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30956a95-173a-48a8-8a2c-ce656c65fc67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Base paths\n",
    "raw_data_base = \"/Volumes/meiproject/default/raw_data\"\n",
    "bronze_base = \"/Volumes/meiproject/default/bronze\"\n",
    "\n",
    "# Helper to create checkpoint folders\n",
    "import os\n",
    "\n",
    "folders = [\n",
    "    f\"{bronze_base}/claims_batch/_checkpoints\",\n",
    "    f\"{bronze_base}/claims_stream/_checkpoints\",\n",
    "    f\"{bronze_base}/diagnosis_ref/_checkpoints\",\n",
    "    f\"{bronze_base}/members_raw/_checkpoints\",\n",
    "    f\"{bronze_base}/providers_raw/_checkpoints\"\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# -------------------------------------\n",
    "# Claims Batch Stream\n",
    "# -------------------------------------\n",
    "claims_batch_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(claims_batch_schema) \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{bronze_base}/claims_batch/_schema\") \\\n",
    "    .load(f\"{raw_data_base}/claims_batch\")\n",
    "\n",
    "claims_batch_query = claims_batch_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{bronze_base}/claims_batch/_checkpoints\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .table(\"default.bronze_claims_batch\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Claims Stream\n",
    "# -------------------------------------\n",
    "claims_stream_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .schema(claims_stream_schema) \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{bronze_base}/claims_stream/_schema\") \\\n",
    "    .load(f\"{raw_data_base}/claims_stream\")\n",
    "\n",
    "claims_stream_query = claims_stream_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{bronze_base}/claims_stream/_checkpoints\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .table(\"default.bronze_claims_stream\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Diagnosis Reference\n",
    "# -------------------------------------\n",
    "diagnosis_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(diagnosis_schema) \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{bronze_base}/diagnosis_ref/_schema\") \\\n",
    "    .load(f\"{raw_data_base}/diagnosis_ref\")\n",
    "\n",
    "diagnosis_query = diagnosis_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{bronze_base}/diagnosis_ref/_checkpoints\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .table(\"default.bronze_diagnosis_ref\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Members\n",
    "# -------------------------------------\n",
    "members_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(members_schema) \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{bronze_base}/members_raw/_schema\") \\\n",
    "    .load(f\"{raw_data_base}/members_raw\")\n",
    "\n",
    "members_query = members_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{bronze_base}/members_raw/_checkpoints\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .table(\"default.bronze_members\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Providers\n",
    "# -------------------------------------\n",
    "providers_stream = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .schema(providers_schema) \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{bronze_base}/providers_raw/_schema\") \\\n",
    "    .load(f\"{raw_data_base}/providers_raw\")\n",
    "\n",
    "providers_query = providers_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{bronze_base}/providers_raw/_checkpoints\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .table(\"default.bronze_providers\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Notes:\n",
    "# 1. All checkpoint folders are pre-created.\n",
    "# 2. Each stream query is assigned to a variable, so you can start/stop individually:\n",
    "#       claims_batch_query.stop()\n",
    "#       claims_stream_query.stop()\n",
    "# 3. Use mergeSchema=True to handle schema evolution safely.\n",
    "# 4. Run each block one by one if needed to avoid overlapping stream errors.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
